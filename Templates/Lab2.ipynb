{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import pickle as pkl\n",
    "# import progressbar\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens        \n",
    "\n",
    "def preprocessing_txt(text):\n",
    "    \n",
    "    tokens = tokenizer(text)\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_text = \"\"\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in stopwords:\n",
    "#             print token\n",
    "            new_text += stemmer.stem(token)\n",
    "            new_text += \" \"\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (753482037.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Sricharan\\AppData\\Local\\Temp\\ipykernel_21684\\753482037.py\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    list_doc = os.listdir(\"C:\\Users\\Sricharan\\Desktop\\information_retrieval\\Data\\cranfieldDocs\")\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "def inverted_index():\n",
    "    \"\"\"\n",
    "    Creates a dictionary of words as key and name of the documents as items\n",
    "    \"\"\"\n",
    "    inverted = {}\n",
    "    docs_indexed = 0\n",
    "    list_doc = os.listdir(\"C:\\Users\\Sricharan\\Desktop\\information_retrieval\\Data\\cranfieldDocs\")\n",
    "    total = len(list_doc)\n",
    "    point = total / 100\n",
    "    increment = total / 100\n",
    "    indexer = {}\n",
    "    for doc in list_doc:\n",
    "#         sys.stdout.write('\\r')\n",
    "        doc_loc = \"C:\\Users\\Sricharan\\Desktop\\information_retrieval\\Data\\cranfieldDocs\" + str(doc)\n",
    "        file_doc = open(doc_loc, \"r\")\n",
    "        file_doc = preprocessing_txt(file_doc.read())\n",
    "        tokens = tokenizer(file_doc)\n",
    "        for word in tokens:\n",
    "            if not inverted.__contains__(word):\n",
    "                count = 1\n",
    "                doclist = {}\n",
    "                doclist[doc] = 1\n",
    "                inverted[word] = doclist\n",
    "            else:\n",
    "                if doc in inverted[word]:\n",
    "                    doclist = inverted[word]\n",
    "                    doclist[doc] += 1\n",
    "                    inverted[word] = doclist\n",
    "                else:\n",
    "                    count = 1\n",
    "                    doclist = inverted[word]\n",
    "                    doclist[doc] = count\n",
    "                    inverted[word] = doclist\n",
    "        docs_indexed += 1\n",
    "        i = docs_indexed\n",
    "        if(i % (point) == 0):\n",
    "            sys.stdout.write(\"\\r[\" + \"=\" * (i / increment) + \">\" +  \" \" * ((total - i)/ increment) + \"]\" +  str(100*i / float(len(list_doc))) + \"%\")\n",
    "            sys.stdout.flush()\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(\"indexed_docs.p\",\"wb\") as handle:\n",
    "        indexed_docs = inverted_index()\n",
    "        pkl.dump(indexed_docs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"indexed_docs.p\",\"r\") as dict_file:\n",
    "    dictionary = pkl.load(dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query_file):\n",
    "    \n",
    "    f = open(query_file,\"r\")\n",
    "    query_ID = []\n",
    "    query_result = []\n",
    "    Time = []\n",
    "    q_res = pd.DataFrame(columns = [\"queryID\",\"Time\"])\n",
    "    for j,query in enumerate(f):\n",
    "        result = []\n",
    "        q_ID = query.split()[0]\n",
    "        query_ID.append(q_ID)\n",
    "        query = preprocessing_txt(query)\n",
    "        query = query.split()\n",
    "        START = time.time()\n",
    "        for i,word in enumerate(query):\n",
    "            \n",
    "            if i == 0:\n",
    "                for i in dictionary[word].items():\n",
    "                    result.append(i[0])\n",
    "            else:\n",
    "                temp = []\n",
    "                for i in dictionary[word].items():\n",
    "                    temp.append(i[0])\n",
    "                result = list(set(result).intersection(set(temp)))\n",
    "        END = time.time()\n",
    "        f1 = open(\"output_inverted_index.txt\",'a')\n",
    "#         print str(q_ID) + \"%d\" % len(result)\n",
    "        for res in result:\n",
    "            f1.write(str(q_ID) + \" \" + str(res) + \"\\n\")\n",
    "        f1.close() \n",
    "        Time.append(float(END - START))\n",
    "        query_result.append(result)\n",
    "    q_res[\"queryID\"] = query_ID\n",
    "    q_res[\"Time\"] = Time\n",
    "    q_res.to_csv(\"inverted_index.csv\",encoding='utf-8')\n",
    "    result = pd.DataFrame(columns = [\"query_ID\",\"relevant_docs\"])\n",
    "    result[\"query_ID\"] = query_ID\n",
    "    result[\"relevant_docs\"] = query_result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query_file = \"query.txt\"\n",
    "    process_query(query_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(output_file,filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        output_file: file containing result for queries\n",
    "    \"\"\"\n",
    "    \n",
    "    output = open(\"output.txt\",'r')\n",
    "    query = open(\"query.txt\",\"r\")\n",
    "    est_output = open(output_file)\n",
    "    query_ID = []\n",
    "    prerec = open(filename,\"a\")\n",
    "    prerec.write(\"queryID\" + \" \" + \"precision\" + \" \" + \"recall\" + \"\\n\")\n",
    "    for line in query:\n",
    "        query_ID.append(line.split()[0])\n",
    "    e_out = pd.DataFrame(columns = [\"q_ID\",\"Doc\"])\n",
    "    o_out = pd.DataFrame(columns=[\"q_ID\",\"Doc\"])\n",
    "    query = []\n",
    "    docs = []\n",
    "    for line in est_output:\n",
    "        query.append(line.split()[0])\n",
    "        docs.append(line.split()[1])\n",
    "    e_out['q_ID'] = query\n",
    "    e_out[\"Doc\"] = docs\n",
    "    query = []\n",
    "    docs = []\n",
    "    for line in output:\n",
    "        query.append(line.split()[0])\n",
    "        docs.append(line.split()[1])\n",
    "    o_out['q_ID'] = query\n",
    "    o_out[\"Doc\"] = docs\n",
    "        \n",
    "    for q_ID in query_ID:\n",
    "        estimated = list(e_out[e_out['q_ID'] == q_ID][\"Doc\"])\n",
    "#         print len(estimated)\n",
    "        true = list(o_out[o_out[\"q_ID\"] == q_ID][\"Doc\"])\n",
    "#         print len(true)\n",
    "\n",
    "        if len(estimated) == 0:\n",
    "            precision = 0\n",
    "        else:    \n",
    "            precision = len(list(set(estimated).intersection(set(true))))/float(len(estimated))\n",
    "        recall = len(list(set(estimated).intersection(set(true))))/float(len(true))\n",
    "        prerec.write(str(q_ID) + \" \" + str(precision) + \" \" + str(recall) + \"\\n\")\n",
    "    prerec.close()\n",
    "    output.close()\n",
    "    est_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15868\\1136784683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprecision_and_recall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output_inverted_index.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"inverted_index_precision_and_recall.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15868\\3757734720.py\u001b[0m in \u001b[0;36mprecision_and_recall\u001b[1;34m(output_file, filename)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"query.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.txt'"
     ]
    }
   ],
   "source": [
    "precision_and_recall(\"output_inverted_index.txt\",\"inverted_index_precision_and_recall.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 16:30:00) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f21c6045f170b622cc5cdf49d23e45d81cd353c1d34e05b260529b713e28859f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
